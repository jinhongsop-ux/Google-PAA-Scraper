import time
import random
import os
import sys
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    StaleElementReferenceException,
    TimeoutException,
    NoSuchElementException,
    ElementClickInterceptedException,
    WebDriverException
)
from selenium_stealth import stealth
from webdriver_manager.chrome import ChromeDriverManager
from collections import deque

# ğŸ”¥ Windows æ§åˆ¶å°ä¸­æ–‡ç¼–ç ä¿®å¤ (é˜²æ­¢ exe è¿è¡Œæ—¶ä¹±ç )
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        # Python < 3.7 å…¼å®¹
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

class PAAScraper:
    def __init__(self, headless=False, max_depth=3):
        self.headless = headless
        self.max_depth = max_depth
        self.results_dir = "results"
        self.data_scraped = []
        self.seen_questions = set()
        self.driver = None
        self.retry_count = 0
        self.max_retries = 3
        
        # Create results directory if it doesn't exist
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
            print(f"Created directory: {self.results_dir}/")
        
        # åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨
        self.driver = self._init_driver_with_retry()

    def setup_driver(self):
        """Configures the Chrome driver with stealth settings."""
        options = Options()
        if self.headless:
            options.add_argument("--headless")
        
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--start-maximized")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-gpu")  # é˜²æ­¢GPUç›¸å…³å´©æºƒ
        options.add_argument("--no-sandbox")  # å¢åŠ å…¼å®¹æ€§
        options.add_argument("--disable-dev-shm-usage")  # é˜²æ­¢å†…å­˜é—®é¢˜
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        # Experimental options to remove automation flags
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶
        driver.set_page_load_timeout(60)
        driver.implicitly_wait(10)

        # Apply selenium-stealth
        stealth(driver,
            languages=["en-US", "en"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
        )
        return driver
    
    def _init_driver_with_retry(self):
        """
        åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨ï¼Œå¸¦é‡è¯•æœºåˆ¶ã€‚
        å¤„ç†ChromeDriverä¸‹è½½å¤±è´¥ã€Chromeæœªå®‰è£…ç­‰é—®é¢˜ã€‚
        """
        last_error = None
        for attempt in range(self.max_retries):
            try:
                print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨... (å°è¯• {attempt + 1}/{self.max_retries})")
                driver = self.setup_driver()
                print("âœ… æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸï¼")
                return driver
            except WebDriverException as e:
                last_error = e
                print(f"âš ï¸  æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)[:100]}")
                if attempt < self.max_retries - 1:
                    print("   ç­‰å¾…5ç§’åé‡è¯•...")
                    time.sleep(5)
            except Exception as e:
                last_error = e
                print(f"âš ï¸  æœªçŸ¥é”™è¯¯: {str(e)[:100]}")
                break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        print("\n" + "="*70)
        print("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼")
        print("="*70)
        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   1. Chromeæµè§ˆå™¨æœªå®‰è£…æˆ–ç‰ˆæœ¬è¿‡æ—§")
        print("   2. ChromeDriverä¸‹è½½å¤±è´¥ï¼ˆæ£€æŸ¥ç½‘ç»œ/ä»£ç†ï¼‰")
        print("   3. æ€æ¯’è½¯ä»¶é˜»æ­¢äº†ChromeDriver")
        print(f"\né”™è¯¯è¯¦æƒ…: {last_error}")
        raise RuntimeError("æ— æ³•åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨")

    def random_sleep(self, min_time=2.0, max_time=5.0):
        """Sleep for a random interval to mimic human behavior."""
        time.sleep(random.uniform(min_time, max_time))

    def scroll_into_view(self, element):
        """Scrolls the element into view."""
        try:
            self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", element)
            time.sleep(random.uniform(0.5, 1.5))
        except Exception:
            pass
    
    def sanitize_filename(self, keyword):
        """Convert keyword to valid filename by removing illegal characters."""
        # Remove or replace illegal filename characters
        illegal_chars = '<>:"/\\|?*'
        filename = keyword
        for char in illegal_chars:
            filename = filename.replace(char, '_')
        # Replace spaces with underscores
        filename = filename.replace(' ', '_')
        # Remove leading/trailing underscores and dots
        filename = filename.strip('_.')
        # Limit length to avoid filesystem issues
        if len(filename) > 200:
            filename = filename[:200]
        return filename
    
    def get_output_filename(self, keyword):
        """Get the output Excel filename for a given keyword."""
        safe_name = self.sanitize_filename(keyword)
        return os.path.join(self.results_dir, f"{safe_name}.xlsx")
    
    def load_historical_data(self, keyword):
        """
        Load historical data for a keyword if it exists.
        Returns the count of loaded questions.
        """
        filename = self.get_output_filename(keyword)
        if os.path.exists(filename):
            try:
                df = pd.read_excel(filename)
                if 'Question/Term' in df.columns:
                    # Load all existing questions into seen_questions set
                    existing_questions = df['Question/Term'].dropna().unique().tolist()
                    self.seen_questions.update(existing_questions)
                    return len(existing_questions)
            except Exception as e:
                print(f"âš ï¸  Error loading historical data: {e}")
        return 0

    def get_paa_questions_elements(self):
        """Locates all PAA question elements currently visible on the page."""
        # Common PAA classes / attributes. 
        # Often div with jsname='Cpkphb' is the question header or div[aria-expanded]
        try:
            # This selector targets the question text container which is usually clickeable
            # We look for elements that look like PAA headers
            elements = self.driver.find_elements(By.CSS_SELECTOR, "div.related-question-pair")
            return elements
        except Exception as e:
            print(f"Error finding PAA elements: {e}")
            return []

    def extract_text(self, element, selector):
        try:
            return element.find_element(By.CSS_SELECTOR, selector).text
        except:
            return ""
    
    def _scroll_to_find_paa(self, max_scrolls=5):
        """
        æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½ï¼Œå¯»æ‰¾PAAå…ƒç´ ã€‚
        Googleæœç´¢ç»“æœæ˜¯æ‡’åŠ è½½çš„ï¼ŒPAAå¯èƒ½åœ¨é¡µé¢ä¸­ä¸‹éƒ¨ã€‚
        
        Args:
            max_scrolls: æœ€å¤§æ»šåŠ¨æ¬¡æ•°
        Returns:
            bool: æ˜¯å¦æ‰¾åˆ°PAAå…ƒç´ 
        """
        # é¦–å…ˆæ£€æŸ¥PAAæ˜¯å¦å·²ç»å­˜åœ¨
        paa_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.related-question-pair")
        if paa_elements:
            return True
        
        # åˆ†æ®µæ»šåŠ¨é¡µé¢ï¼Œè§¦å‘æ‡’åŠ è½½
        scroll_pause_time = 1.5
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        for i in range(max_scrolls):
            # æ»šåŠ¨ä¸€å±çš„é«˜åº¦ï¼ˆçº¦500åƒç´ ï¼‰
            scroll_amount = 500 * (i + 1)
            self.driver.execute_script(f"window.scrollTo(0, {scroll_amount});")
            time.sleep(scroll_pause_time)
            
            # æ¯æ¬¡æ»šåŠ¨åæ£€æŸ¥PAAæ˜¯å¦å‡ºç°
            paa_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.related-question-pair")
            if paa_elements:
                print(f"   âœ“ åœ¨ç¬¬ {i + 1} æ¬¡æ»šåŠ¨åæ‰¾åˆ° PAA")
                # æ»šåŠ¨å›PAAä½ç½®
                self.scroll_into_view(paa_elements[0])
                return True
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ°åº•éƒ¨
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if scroll_amount >= new_height:
                break
        
        # æ»šåŠ¨å›é¡¶éƒ¨å†åšæœ€åä¸€æ¬¡æ£€æŸ¥
        self.driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        
        paa_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.related-question-pair")
        return len(paa_elements) > 0
    
    def _safe_click(self, element, max_attempts=3):
        """
        å®‰å…¨ç‚¹å‡»å…ƒç´ ï¼Œå¤„ç†å„ç§ç‚¹å‡»å¤±è´¥çš„æƒ…å†µã€‚
        
        Args:
            element: è¦ç‚¹å‡»çš„å…ƒç´ 
            max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°
        Returns:
            bool: æ˜¯å¦æˆåŠŸç‚¹å‡»
        """
        for attempt in range(max_attempts):
            try:
                # å…ˆæ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
                self.scroll_into_view(element)
                time.sleep(0.3)
                
                # å°è¯•æ™®é€šç‚¹å‡»
                element.click()
                return True
                
            except ElementClickInterceptedException:
                # ç‚¹å‡»è¢«å…¶ä»–å…ƒç´ é®æŒ¡ï¼Œå°è¯•ç”¨JavaScriptç‚¹å‡»
                try:
                    self.driver.execute_script("arguments[0].click();", element)
                    return True
                except:
                    pass
                    
            except StaleElementReferenceException:
                # å…ƒç´ å·²è¿‡æœŸï¼Œæ— æ³•é‡è¯•
                return False
                
            except Exception as e:
                if attempt < max_attempts - 1:
                    time.sleep(0.5)
                    continue
                else:
                    return False
        
        return False
    
    def _check_for_captcha(self):
        """
        æ£€æŸ¥é¡µé¢æ˜¯å¦å‡ºç°CAPTCHAæˆ–äººæœºéªŒè¯ã€‚
        æ”¯æŒå¤šç§GoogleéªŒè¯ç±»å‹ã€‚
        
        Returns:
            bool: æ˜¯å¦æ£€æµ‹åˆ°éªŒè¯
        """
        page_source = self.driver.page_source.lower()
        current_url = self.driver.current_url.lower()
        
        captcha_indicators = [
            "captcha",
            "recaptcha",
            "unusual traffic",
            "automated queries",
            "sorry/index",
            "ipv4.google.com/sorry"
        ]
        
        for indicator in captcha_indicators:
            if indicator in page_source or indicator in current_url:
                return True
        
        return False

    def process_keyword(self, keyword):
        """
        å¤„ç†å•ä¸ªå…³é”®è¯ï¼Œæ”¯æŒ PAA è§¦å‘å¤±è´¥åçš„é‡è¯•æœºåˆ¶ã€‚
        """
        print(f"\n{'='*70}")
        print(f"ğŸ” Processing Keyword: {keyword}")
        print(f"{'='*70}")
        
        # Reset seen_questions and load historical data
        self.seen_questions = set()
        historical_count = self.load_historical_data(keyword)
        
        if historical_count > 0:
            print(f"ğŸ“š å·²åŠ è½½å…³é”®è¯ [{keyword}] çš„å†å²æ•°æ® {historical_count} æ¡ï¼Œå°†è‡ªåŠ¨è·³è¿‡é‡å¤...")
        else:
            print(f"ğŸ“ å…³é”®è¯ [{keyword}] æ— å†å²æ•°æ®ï¼Œå¼€å§‹å…¨æ–°æŠ“å–...")
        
        # Store current keyword for saving (å§‹ç»ˆä½¿ç”¨åŸå§‹å…³é”®è¯ä½œä¸ºæ–‡ä»¶å)
        self.current_keyword = keyword
        
        # ğŸ”¥ PAA è§¦å‘é‡è¯•å‰ç¼€åˆ—è¡¨
        retry_prefixes = [
            "",                    # åŸå§‹å…³é”®è¯
            "What is ",            # å®šä¹‰ç±»
            "Best ",               # æµ‹è¯„ç±» / ç”µå•†ç±»
            "How to use ",         # æ•™ç¨‹ç±»
            "How to choose ",      # é€‰è´­ç±»
            " guide",              # æŒ‡å—ç±» (åç¼€)
        ]
        
        # å°è¯•ä¸åŒçš„å…³é”®è¯å˜ä½“è§¦å‘ PAA
        for i, prefix in enumerate(retry_prefixes):
            # æ„é€ æœç´¢è¯
            if prefix.endswith(" "):
                search_term = prefix + keyword
            elif prefix.startswith(" "):
                search_term = keyword + prefix  # åç¼€æ¨¡å¼
            else:
                search_term = keyword
            
            is_retry = (i > 0)
            
            if is_retry:
                print(f"\n{'~'*50}")
                print(f"ğŸ”„ åŸå§‹è¯æœªè§¦å‘ PAAï¼Œå°è¯•å˜ä½“ #{i}: '{search_term}'")
                print(f"{'~'*50}")
            
            # æ‰§è¡Œæœç´¢å’ŒæŠ“å–
            success = self._search_and_scrape_paa(
                search_term=search_term,
                original_keyword=keyword,
                is_retry=is_retry
            )
            
            if success:
                if is_retry:
                    print(f"âœ… å˜ä½“å…³é”®è¯ '{search_term}' æˆåŠŸè§¦å‘ PAAï¼")
                return  # æˆåŠŸï¼Œç»“æŸå¤„ç†
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        print(f"\n{'!'*50}")
        print(f"âŒ å…³é”®è¯ '{keyword}' å½»åº•æœªæ‰¾åˆ° PAA")
        print(f"{'!'*50}")
        print("ğŸ’¡ å»ºè®®ï¼š")
        print("   1. è¯¥å…³é”®è¯å¯èƒ½ç¡®å®æ²¡æœ‰ PAA ç»“æœ")
        print("   2. å°è¯•ä½¿ç”¨æ›´å…·ä½“æˆ–æ›´é€šç”¨çš„å…³é”®è¯")
        print("   3. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®")
    
    def _search_and_scrape_paa(self, search_term, original_keyword, is_retry=False):
        """
        æ‰§è¡Œæœç´¢å¹¶æŠ“å– PAA çš„æ ¸å¿ƒé€»è¾‘ã€‚
        
        Args:
            search_term: å®é™…æœç´¢çš„å…³é”®è¯ï¼ˆå¯èƒ½å¸¦å‰ç¼€ï¼‰
            original_keyword: åŸå§‹å…³é”®è¯ï¼ˆç”¨äºæ•°æ®è®°å½•ï¼‰
            is_retry: æ˜¯å¦ä¸ºé‡è¯•æ¨¡å¼
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸæŠ“å–åˆ° PAA
        """
        # å¯¼èˆªåˆ° Google
        self.driver.get("https://www.google.com")
        self.random_sleep(1, 2)

        # Search
        try:
            search_box = self.driver.find_element(By.NAME, "q")
            search_box.clear()
            search_box.send_keys(search_term)
            self.random_sleep(0.5, 1)
            search_box.submit()
        except Exception as e:
            print(f"Error during search: {e}")
            return False

        self.random_sleep(2, 4)

        # Basic Check for CAPTCHA (æ”¯æŒå¤šç§éªŒè¯ç±»å‹)
        if self._check_for_captcha():
            print("\n" + "!"*50)
            print("ğŸ”’ æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼")
            print("!"*50)
            input("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯ï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")
            print("â³ ç­‰å¾…é¡µé¢åˆ·æ–°ä¸­ï¼Œè¯·ç¨å€™...")
            time.sleep(5)
            self.random_sleep(2, 3)
            
            if self._check_for_captcha():
                print("âš ï¸  éªŒè¯ä¼¼ä¹æœªå®Œæˆï¼Œè¯·é‡æ–°éªŒè¯åå†è¯•")
                return False

        # ğŸ”¥ æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½ï¼Œç¡®ä¿PAAå…ƒç´ è¢«åŠ è½½
        print("ğŸ“œ æ»šåŠ¨é¡µé¢ä»¥åŠ è½½å…¨éƒ¨å†…å®¹...")
        paa_found = self._scroll_to_find_paa()
        
        if not paa_found:
            if not is_retry:
                print(f"âš ï¸ åŸå§‹è¯ '{search_term}' æœªæ‰¾åˆ° PAAï¼Œå‡†å¤‡å°è¯•å˜ä½“...")
            return False
        
        print("âœ… PAA section found! Starting extraction...")
        
        # è®°å½•æ•°æ®æ¥æºæ ‡è®°
        source_tag = f"[é‡è¯•: {search_term}]" if is_retry else ""
        
        # Recursive Expansion
        iteration_count = 0
        total_extracted = 0
        
        while iteration_count < self.max_depth:
            print(f"Expansion Level {iteration_count + 1}...")
            
            # Find all current PAA elements
            paa_pairs = self.driver.find_elements(By.CSS_SELECTOR, "div.related-question-pair")
            
            clicks_made_this_round = 0
            
            for pair in paa_pairs:
                try:
                    # Extract Question Text
                    question_div = pair.find_element(By.CSS_SELECTOR, "div[role='button']")
                    question_text = question_div.text
                    
                    if not question_text or question_text in self.seen_questions:
                        continue
                    
                    self.seen_questions.add(question_text)

                    # Scroll and Click to expand
                    self.scroll_into_view(question_div)
                    
                    # Check if already expanded
                    is_expanded = question_div.get_attribute("aria-expanded")
                    if is_expanded == "false":
                        if self._safe_click(question_div):
                            clicks_made_this_round += 1
                            self.random_sleep(2, 4)
                        else:
                            print(f"   âš ï¸  ç‚¹å‡»å¤±è´¥ï¼Œè·³è¿‡: {question_text[:30]}...")
                            continue
                    
                    # Try to find snippet and link
                    snippet = ""
                    source_link = ""
                    
                    try:
                        snippet_el = pair.find_element(By.CSS_SELECTOR, ".wDYxhc")
                        snippet = snippet_el.text
                        link_el = snippet_el.find_element(By.CSS_SELECTOR, "a")
                        source_link = link_el.get_attribute("href")
                    except Exception:
                        try:
                            blocks = pair.find_elements(By.CSS_SELECTOR, "div")
                            for b in blocks:
                                if len(b.text) > 20 and b.text != question_text:
                                    snippet = b.text
                                    break
                        except:
                            pass

                    # Create data record - æ·»åŠ æ¥æºæ ‡è®°
                    data_record = {
                        "Original Keyword": original_keyword,
                        "Search Term": search_term if is_retry else original_keyword,
                        "Type": "PAA",
                        "Question/Term": question_text,
                        "Snippet": snippet,
                        "Source Link": source_link,
                        "Discovery Level": iteration_count + 1,
                        "Data Source": "Retry" if is_retry else "Original"
                    }
                    
                    self.data_scraped.append(data_record)
                    self.save_to_excel(new_data_only=data_record)
                    total_extracted += 1
                    
                except StaleElementReferenceException:
                    print("âš ï¸  Stale Element encountered. Skipping...")
                    continue
                except Exception as e:
                    print(f"âš ï¸  Error processing element: {str(e)[:100]}. Continuing...")
                    continue
            
            if clicks_made_this_round == 0:
                print("No new unique questions clicked this round. Stopping expansion.")
                break
                
            iteration_count += 1
            self.random_sleep(1, 3)

        # Extraction of 'People also search for'
        print("Extracting 'People also search for'...")
        try:
            related_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.s75CSd, div.k8XOCe, a.k8XOCe")
            if not related_elements:
                 related_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.AJLUJb")

            count = 0 
            for el in related_elements:
                try:
                    text = el.text
                    href = el.get_attribute("href")
                    if not href:
                        try:
                            href = el.find_element(By.TAG_NAME, "a").get_attribute("href")
                        except:
                            pass
                    
                    if text and text not in self.seen_questions:
                         data_record = {
                            "Original Keyword": original_keyword,
                            "Search Term": search_term if is_retry else original_keyword,
                            "Type": "Related Search",
                            "Question/Term": text,
                            "Snippet": "",
                            "Source Link": href if href else "",
                            "Discovery Level": 0,
                            "Data Source": "Retry" if is_retry else "Original"
                        }
                         self.data_scraped.append(data_record)
                         self.save_to_excel(new_data_only=data_record)
                         count += 1
                         total_extracted += 1
                except:
                    continue
            print(f"Captured {count} related search terms.")

        except Exception as e:
            print(f"Error extracting related searches: {e}")
        
        # è¿”å›æ˜¯å¦æˆåŠŸæŠ“å–åˆ°æ•°æ®
        return total_extracted > 0

    def save_to_excel(self, new_data_only=None):
        """
        Save data to Excel with incremental append and deduplication.
        Uses per-keyword filenames based on self.current_keyword.
        
        Args:
            new_data_only: If provided, save only this single record (for checkpoint saving)
        """
        # Get filename for current keyword
        if not hasattr(self, 'current_keyword'):
            print("âš ï¸  No current keyword set. Cannot save.")
            return
        
        filename = self.get_output_filename(self.current_keyword)
        
        # Determine what data to save
        if new_data_only:
            new_df = pd.DataFrame([new_data_only])
        elif self.data_scraped:
            new_df = pd.DataFrame(self.data_scraped)
        else:
            print("No data to save.")
            return

        try:
            # Check if file exists and read existing data
            if os.path.exists(filename):
                existing_df = pd.read_excel(filename)
                # Combine existing and new data
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            else:
                combined_df = new_df
            
            # Remove duplicates based on Question/Term (keep first occurrence)
            combined_df = combined_df.drop_duplicates(subset=["Question/Term"], keep="first")
            
            # Save to Excel
            combined_df.to_excel(filename, index=False)
            
            if new_data_only:
                # Checkpoint save - show progress
                total_count = len(combined_df)
                print(f"âœ“ å·²ä¿å­˜ï¼š{new_data_only['Question/Term'][:60]}... | å½“å‰æ€»æ•°ï¼š{total_count}æ¡")
            else:
                print(f"Data saved to {filename} (Total: {len(combined_df)} records)")
                
        except Exception as e:
            print(f"Error saving to Excel: {e}. Attempting backup save...")
            try:
                backup_filename = f"paa_results_backup_{int(time.time())}.xlsx"
                new_df.to_excel(backup_filename, index=False)
                print(f"Backup saved to {backup_filename}")
            except Exception as backup_error:
                print(f"Backup save also failed: {backup_error}")

    def quit(self):
        self.driver.quit()

if __name__ == "__main__":
    import json
    
    print("="*70)
    print("ğŸ”§ PAA Scraper - Google People Also Ask æŠ“å–å·¥å…·")
    print("="*70)
    
    config_path = "config.json"
    if not os.path.exists(config_path):
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œæ­£åœ¨åˆ›å»ºé»˜è®¤é…ç½®...")
        default_config = {
            "keywords": ["python automation"],
            "max_depth": 3,
            "headless": False,
            "output_file": "paa_results.xlsx"
        }
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(default_config, f, indent=4, ensure_ascii=False)
        print("âœ… å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ï¼Œè¯·ç¼–è¾‘ config.json åé‡æ–°è¿è¡Œã€‚")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit()

    # è¯»å–é…ç½®æ–‡ä»¶ï¼Œå¸¦å¼‚å¸¸å¤„ç†
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
    except json.JSONDecodeError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼")
        print(f"   é”™è¯¯ä½ç½®: ç¬¬ {e.lineno} è¡Œï¼Œç¬¬ {e.colno} åˆ—")
        print(f"   é”™è¯¯è¯¦æƒ…: {e.msg}")
        print("\nğŸ’¡ è¯·æ£€æŸ¥ config.json çš„ JSON æ ¼å¼æ˜¯å¦æ­£ç¡®")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)

    keywords = config.get("keywords", ["python automation"])
    max_depth = config.get("max_depth", 3)
    headless = config.get("headless", False)

    print(f"Loaded {len(keywords)} keywords from config.")
    print(f"Max Depth: {max_depth}, Headless: {headless}")

    scraper = PAAScraper(headless=headless, max_depth=max_depth)
    
    try:
        for kw in keywords:
            scraper.process_keyword(kw)
        
        # æˆåŠŸå®Œæˆ
        print("\n" + "="*70)
        print("ğŸ‰ All keywords processed! Results saved in 'results/' folder.")
        print("="*70)
        
    except Exception as e:
        # å…¨å±€å¼‚å¸¸æ•è·ï¼Œé˜²æ­¢é—ªé€€
        print("\n" + "="*70)
        print("âŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼")
        print("="*70)
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        print("\nğŸ’¡ å¸¸è§è§£å†³æ–¹æ¡ˆ:")
        print("   1. å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œè¯·æ£€æŸ¥VPN/ä»£ç†æ˜¯å¦æ­£å¸¸")
        print("   2. å¦‚æœæ˜¯å…ƒç´ æ‰¾ä¸åˆ°ï¼Œå¯èƒ½Googleé¡µé¢ç»“æ„æœ‰å˜åŒ–")
        print("   3. å¦‚æœé¢‘ç¹å‡ºç°CAPTCHAï¼Œè¯·é™ä½æŠ“å–é¢‘ç‡")
        import traceback
        print("\n--- å®Œæ•´é”™è¯¯å †æ ˆ ---")
        traceback.print_exc()
        
    finally:
        try:
            scraper.quit()
        except:
            pass
        
        # ğŸ”¥ é˜²é—ªé€€ï¼šç¨‹åºç»“æŸå‰æš‚åœ
        print("\n" + "-"*70)
        input("ğŸ“Œ ç¨‹åºå·²ç»“æŸã€‚æŒ‰å›è½¦é”®å…³é—­çª—å£...")
